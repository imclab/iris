* TODO Get something working.
  #+BEGIN_SRC java :tangle working.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/stt.jar");
    addClassPath("lib/minim.jar");
    addClassPath("lib/minim-spi.jar");
    addClassPath("lib/jsminim.jar");
    addClassPath("lib/tritonus_share.jar");
    addClassPath("lib/javaFlacEncoder-0.1.jar");
    addClassPath("lib/core.jar");
    
    import com.getflourish.stt.STT;
    import processing.core.PApplet;
    
    new PApplet() {
            public setup() {
                // size(400, 400);
                print("oeunthouethn");
                noLoop();
            }
    
            draw() {
                // background(0);
            }
    
            transcribe(utterance, confidence) {
            }
    
            keyPressed() {
            }
    
            keyReleased() {
            }
        };
    
    stt = new STT(applet);
    
  #+END_SRC

  If we're going to do this without the autorecord and processing
  cruft, we need (I was going to say [[http://code.compartmental.net/tools/minim/][minim]], but it's some kind of
  Processing-specific piece of shit) [[http://www.tritonus.org/][tritonus]].

  On the [[https://github.com/fx-lange/ofxGSTT][C-side]], on the other hand, there's [[http://www.mega-nerd.com/libsndfile/][sndfile]] and [[http://flac.sourceforge.net/][libFlac]]; looks
  like [[http://freedesktop.org/software/pulseaudio/doxygen/simple.html][pulseaudio]]'s the way to go, though, for actually recording.

  What about [[http://www.jsresources.org/examples/audio_playing_recording.html][this shit]] on Java? Or [[http://docs.oracle.com/javase/tutorial/sound/accessing.html][from scratch]]. Write with [[http://javaflacencoder.sourceforge.net/][this]]?
  [[http://www.jsresources.org/examples/audio_playing_recording.html][Examples]] of recording to file.

  Now that we have an =AudioInputStream=, can we avoid serializing it
  before converting to FLAC? =AudioSystem.write= takes an
  =OutputStream=, by the way.

  #+BEGIN_SRC java :tangle mixer.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/guava-10.0.1.jar");
    addClassPath("lib/javaFlacEncoder-0.2.3.jar");
    addClassPath("lib/jflac-codec-1.4.0-SNAPSHOT.jar");
    
    import javax.sound.sampled.AudioSystem;
    import javax.sound.sampled.Port;
    import javax.sound.sampled.TargetDataLine;
    import javax.sound.sampled.DataLine;
    import javax.sound.sampled.AudioFormat;
    import javax.sound.sampled.AudioInputStream;
    import javax.sound.sampled.AudioFileFormat;
    import java.util.Timer;
    import java.util.TimerTask;
    import java.io.ByteArrayOutputStream;
    
    import com.google.common.collect.ObjectArrays;
    import javaFlacEncoder.FLACFileOutputStream;
    import org.kc7bfi.jflac.sound.spi.FlacFileFormatType;
    import org.kc7bfi.jflac.sound.spi.FlacEncoding;
    
    // It's a shame we have to specify this: command-line param?
    INPUT_INDEX = 1;
    FORMAT = new AudioFormat((float) 8000.0, 8, 1, true, false);
    
    mixerInfo = AudioSystem.getMixerInfo()[INPUT_INDEX];
    target = AudioSystem.getTargetDataLine(FORMAT, mixerInfo);
    target.open(FORMAT);
    target.start();
    
    timer = new Timer();
    task = new TimerTask() {
            public void run() {
                // Otherwise, our WAV is truncated.
                target.flush();
                target.stop();
                target.close();
                // Otherwise, the program never terminates.
                timer.cancel();
            }
        };
    timer.schedule(task, 1000);
    
    inputStream = new AudioInputStream(target);
    AudioSystem.write(inputStream,
                      AudioFileFormat.Type.WAVE,
                      new File("harro.wav"));
    
  #+END_SRC

  This works, by the way (based on [[http://getstreaming.wordpress.com/tag/speech-to-text/][this]]):

  #+BEGIN_SRC sh
    curl -H "Content-Type: audio/x-flac; rate=16000" -F Content=@harro.flac -k 'https://www.google.com/speech-api/v1/recognize?xjerr=1&client=chromium&lang=en-US'
    # {"status":0,"id":"fa71c13664c1b6804bd7f2ef84a2a4e0-1","hypotheses":[{"utterance":"test","confidence":0.95221627}]}
  #+END_SRC

  Having been converted with this:

  #+BEGIN_SRC sh
    sox harro.wav -2 -r 16000 harro.flac
  #+END_SRC

  [[http://www.developer.com/java/other/article.php/2105421/Java-Sound-Capturing-Microphone-Data-into-an-Audio-File.htm][By the way]]:

  #+BEGIN_QUOTE
: In addition to its other features, the AudioSystem.write method knows
: how to detect that the stop method has been invoked on the
: TargetDataLine object (see Listing 7) and to close the output file
: when that happens.  
  #+END_QUOTE

  It would be pretty cool to detect starts and stops in the sound
  stream and not have to rely on e.g. timers and button-events; this
  can be a later optimization, though (also, take a look at the source
  for Florian Schulz' [[http://stt.getflourish.com/][Processing-plugin]]).

  We should have an alternative, by the way, that pulls in the first
  compatible =TargetDataLine= (and only resorts to a specific index
  when necessary); in other words, it should be possible to specify
  the default source and call it a day (though this didn't work for us
  using PulseAudio).

  Florian Schulz even did things like the "analysis of the
  environmental volume after initialization" (which appears to take
  the max volume over a two-second interval; discarding the average,
  AFAICT):

  #+BEGIN_SRC java
    private void analyzeEnv() {
        if (!analyzing) {
            timer2 = new Timer(2000);
            timer2.start();
            analyzing = true;
            volumes = new ArrayList<Float>();
        }
        if (timer2 != null) {
            if (!timer2.isFinished()) {
                float volume = in.mix.level() * 1000;
                volumes.add(volume);
            } else {
                float avg = 0.0f;
                float max = 0.0f;
                for (int i = 0; i < volumes.size(); i++) {
                    avg += volumes.get(i);
                    if (volumes.get(i) > max) max = volumes.get(i);
                }
                avg /= volumes.size();
                threshold = (float) Math.ceil(max);
                System.out.println(getTime() + " Volume threshold automatically set to " + threshold);
                analyzing = false;
            }   
        }   
    }
  #+END_SRC

  Look at the encoding from Wave to FLAC, by the way:

  #+BEGIN_SRC java
    private void onSpeechFinish()
    {
        status = "Transcribing";
        fired = false;
        recorder.endRecord();
        recorder.save();
        recording = false;
            
        dispatchTranscriptionEvent(transcriptionThread.getUtterance(), transcriptionThread.getConfidence(), STT.TRANSCRIBING);
            
        // Encode the wav to flac
        String flac = path + fileName + fileCount + ".flac";
        encoder.encode(new File(path + fileName + fileCount + ".wav"), new File(flac));
        boolean exists = (new File(flac)).exists();
        while(exists == false)
            {   
                exists = (new File(flac)).exists();     
            }
        
        if (exists) {
            this.transcribe(flac);
        } else {
            System.err.println("Could not transcribe. File was not encoded in time.");
        }
            
        // new file for new speech
        if (log) fileCount++;
    }
    
  #+END_SRC

  Here's the =handleAuto= loop: where it analyses the environment,
  sets up the threshould, and dispatches:

  #+BEGIN_SRC java
    private void handleAuto () {
        if (analyzing) analyzeEnv();
        updateVolume(); 
        if (volume > threshold) {
            // start recording when someone says something louder than threshold
            onSpeech();
        } else {
            // the magic begins. save it. transcribe it.
            if (timer.isFinished() && volume < threshold && recorder.isRecording() && recording) {
                onSpeechFinish();
            } else if (timer.isFinished() && volume < threshold && !recorder.isRecording()){
                startListening();
            }
        }
    }
    
  #+END_SRC

  No FFT, though; [[https://github.com/taf2/audiosplit][audiosplit]], on the other hand, is doing some kind of
  root-mean-square analysis. =handleAuto= is called everytime there's
  a draw-event, by the way:

  #+BEGIN_SRC java
    public void draw() {    
        if (auto) handleAuto();
        // handles active threads and callbacks
        for (int i = 0; i < threads.size(); i++) {
            transcriptionThread = threads.get(i); 
            transcriptionThread.debug = debug;
            if (transcriptionThread.isAvailable()) {
                if (transcriptionEvent != null) {
                    try {
                        transcriptionEvent.invoke(p, new Object[] { transcriptionThread.getUtterance(), transcriptionThread.getConfidence()});
                    } catch (IllegalArgumentException e) {
                        // TODO Auto-generated catch block
                        e.printStackTrace();
                    } catch (IllegalAccessException e) {
                        // TODO Auto-generated catch block
                        e.printStackTrace();
                    } catch (InvocationTargetException e) {
    
                    }
                } else if (transcriptionEvent2 != null) {
                    dispatchTranscriptionEvent(transcriptionThread.getUtterance(), transcriptionThread.getConfidence(), transcriptionThread.getStatus());
                }
                threads.remove(i);
            }
    
            if (debug && !status.equals(lastStatus)) {
                System.out.println(getTime() + " " + status);
                lastStatus = status;
            }
        }
    }
    
  #+END_SRC

  Call-back for the reduction-event is: =(lambda (hypothesis
  confidence) ...)=; register a series of parsers which either bite or
  pass on. Initially, though, just a parser. Or: one parser; multiple
  dispatchers? Yes.
