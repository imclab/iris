* TODO Get something working.
  #+BEGIN_SRC java :tangle working.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/stt.jar");
    addClassPath("lib/minim.jar");
    addClassPath("lib/minim-spi.jar");
    addClassPath("lib/jsminim.jar");
    addClassPath("lib/tritonus_share.jar");
    addClassPath("lib/javaFlacEncoder-0.1.jar");
    addClassPath("lib/core.jar");
    
    import com.getflourish.stt.STT;
    import processing.core.PApplet;
    
    new PApplet() {
            public setup() {
                // size(400, 400);
                print("oeunthouethn");
                noLoop();
            }
    
            draw() {
                // background(0);
            }
    
            transcribe(utterance, confidence) {
            }
    
            keyPressed() {
            }
    
            keyReleased() {
            }
        };
    
    stt = new STT(applet);
    
  #+END_SRC

  If we're going to do this without the autorecord and processing
  cruft, we need (I was going to say [[http://code.compartmental.net/tools/minim/][minim]], but it's some kind of
  Processing-specific piece of shit) [[http://www.tritonus.org/][tritonus]].

  On the [[https://github.com/fx-lange/ofxGSTT][C-side]], on the other hand, there's [[http://www.mega-nerd.com/libsndfile/][sndfile]] and [[http://flac.sourceforge.net/][libFlac]]; looks
  like [[http://freedesktop.org/software/pulseaudio/doxygen/simple.html][pulseaudio]]'s the way to go, though, for actually recording.

  What about [[http://www.jsresources.org/examples/audio_playing_recording.html][this shit]] on Java? Or [[http://docs.oracle.com/javase/tutorial/sound/accessing.html][from scratch]]. Write with [[http://javaflacencoder.sourceforge.net/][this]]?
  [[http://www.jsresources.org/examples/audio_playing_recording.html][Examples]] of recording to file.

  Now that we have an =AudioInputStream=, can we avoid serializing it
  before converting to FLAC? =AudioSystem.write= takes an
  =OutputStream=, by the way.

  #+BEGIN_SRC java :tangle mixer.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/guava-10.0.1.jar");
    addClassPath("lib/javaFlacEncoder-0.2.3.jar");
    addClassPath("lib/jflac-codec-1.4.0-SNAPSHOT.jar");
    
    import javax.sound.sampled.AudioSystem;
    import javax.sound.sampled.Port;
    import javax.sound.sampled.TargetDataLine;
    import javax.sound.sampled.DataLine;
    import javax.sound.sampled.AudioFormat;
    import javax.sound.sampled.AudioInputStream;
    import javax.sound.sampled.AudioFileFormat;
    import java.util.Timer;
    import java.util.TimerTask;
    import java.io.ByteArrayOutputStream;
    
    import com.google.common.collect.ObjectArrays;
    import javaFlacEncoder.FLACFileOutputStream;
    import javaFlacEncoder.FLAC_FileEncoder;
    import javaFlacEncoder.StreamConfiguration;
    import org.kc7bfi.jflac.sound.spi.FlacEncoding;
    import org.kc7bfi.jflac.sound.spi.FlacFileFormatType;
    import org.kc7bfi.jflac.sound.spi.FlacFormatConversionProvider;
    
    // It's a shame we have to specify this: command-line param?
    INPUT_INDEX = 1;
    FORMAT = new AudioFormat(8000, 16, 1, true, false);
    
    mixerInfo = AudioSystem.getMixerInfo()[INPUT_INDEX];
    target = AudioSystem.getTargetDataLine(FORMAT, mixerInfo);
    target.open(FORMAT);
    target.start();
    
    timer = new Timer();
    task = new TimerTask() {
            public void run() {
                // Otherwise, our WAV is truncated.
                target.flush();
                target.stop();
                target.close();
                // Otherwise, the program never terminates.
                timer.cancel();
            }
        };
    timer.schedule(task, 10000);
    
    inputStream = new AudioInputStream(target);
    
    wave = new File("harro.wav");
    flac = new File("harro.flac");
    
    AudioSystem.write(inputStream,
                      AudioFileFormat.Type.WAVE,
                      wave);
    
    encoder = new FLAC_FileEncoder();
    encoder.setStreamConfig
        (new StreamConfiguration(1,
                                 StreamConfiguration.DEFAULT_MIN_BLOCK_SIZE,
                                 StreamConfiguration.DEFAULT_MAX_BLOCK_SIZE,
                                 8000,
                                 16));
    encoder.encode(wave, flac);
    
  #+END_SRC

  This works, by the way (based on [[http://getstreaming.wordpress.com/tag/speech-to-text/][this]]):

  #+BEGIN_SRC sh
    curl -H "Content-Type: audio/x-flac; rate=16000" -F Content=@harro.flac -k 'https://www.google.com/speech-api/v1/recognize?xjerr=1&client=chromium&lang=en-US'
    # {"status":0,"id":"fa71c13664c1b6804bd7f2ef84a2a4e0-1","hypotheses":[{"utterance":"test","confidence":0.95221627}]}
  #+END_SRC

  Having been converted with this:

  #+BEGIN_SRC sh
    sox harro.wav -2 -r 16000 harro.flac
  #+END_SRC

  [[http://www.developer.com/java/other/article.php/2105421/Java-Sound-Capturing-Microphone-Data-into-an-Audio-File.htm][By the way]]:

  #+BEGIN_QUOTE
: In addition to its other features, the AudioSystem.write method knows
: how to detect that the stop method has been invoked on the
: TargetDataLine object (see Listing 7) and to close the output file
: when that happens.  
  #+END_QUOTE

  It would be pretty cool to detect starts and stops in the sound
  stream and not have to rely on e.g. timers and button-events; this
  can be a later optimization, though (also, take a look at the source
  for Florian Schulz' [[http://stt.getflourish.com/][Processing-plugin]]).

  We should have an alternative, by the way, that pulls in the first
  compatible =TargetDataLine= (and only resorts to a specific index
  when necessary); in other words, it should be possible to specify
  the default source and call it a day (though this didn't work for us
  using PulseAudio).

  Florian Schulz even did things like the "analysis of the
  environmental volume after initialization" (which appears to take
  the max volume over a two-second interval; discarding the average,
  AFAICT):

  #+BEGIN_SRC java
    private void analyzeEnv() {
        if (!analyzing) {
            timer2 = new Timer(2000);
            timer2.start();
            analyzing = true;
            volumes = new ArrayList<Float>();
        }
        if (timer2 != null) {
            if (!timer2.isFinished()) {
                float volume = in.mix.level() * 1000;
                volumes.add(volume);
            } else {
                float avg = 0.0f;
                float max = 0.0f;
                for (int i = 0; i < volumes.size(); i++) {
                    avg += volumes.get(i);
                    if (volumes.get(i) > max) max = volumes.get(i);
                }
                avg /= volumes.size();
                threshold = (float) Math.ceil(max);
                System.out.println(getTime() + " Volume threshold automatically set to " + threshold);
                analyzing = false;
            }   
        }   
    }
  #+END_SRC

  Look at the encoding from Wave to FLAC, by the way:

  #+BEGIN_SRC java
    private void onSpeechFinish()
    {
        status = "Transcribing";
        fired = false;
        recorder.endRecord();
        recorder.save();
        recording = false;
            
        dispatchTranscriptionEvent(transcriptionThread.getUtterance(), transcriptionThread.getConfidence(), STT.TRANSCRIBING);
            
        // Encode the wav to flac
        String flac = path + fileName + fileCount + ".flac";
        encoder.encode(new File(path + fileName + fileCount + ".wav"), new File(flac));
        boolean exists = (new File(flac)).exists();
        while(exists == false)
            {   
                exists = (new File(flac)).exists();     
            }
        
        if (exists) {
            this.transcribe(flac);
        } else {
            System.err.println("Could not transcribe. File was not encoded in time.");
        }
            
        // new file for new speech
        if (log) fileCount++;
    }
    
  #+END_SRC

  Here's the =handleAuto= loop: where it analyses the environment,
  sets up the threshould, and dispatches:

  #+BEGIN_SRC java
    private void handleAuto () {
        if (analyzing) analyzeEnv();
        updateVolume(); 
        if (volume > threshold) {
            // start recording when someone says something louder than threshold
            onSpeech();
        } else {
            // the magic begins. save it. transcribe it.
            if (timer.isFinished() && volume < threshold && recorder.isRecording() && recording) {
                onSpeechFinish();
            } else if (timer.isFinished() && volume < threshold && !recorder.isRecording()){
                startListening();
            }
        }
    }
    
  #+END_SRC

  No FFT, though; [[https://github.com/taf2/audiosplit][audiosplit]], on the other hand, is doing some kind of
  root-mean-square analysis. =handleAuto= is called everytime there's
  a draw-event, by the way:

  #+BEGIN_SRC java
    public void draw() {    
        if (auto) handleAuto();
        // handles active threads and callbacks
        for (int i = 0; i < threads.size(); i++) {
            transcriptionThread = threads.get(i); 
            transcriptionThread.debug = debug;
            if (transcriptionThread.isAvailable()) {
                if (transcriptionEvent != null) {
                    try {
                        transcriptionEvent.invoke(p, new Object[] { transcriptionThread.getUtterance(), transcriptionThread.getConfidence()});
                    } catch (IllegalArgumentException e) {
                        // TODO Auto-generated catch block
                        e.printStackTrace();
                    } catch (IllegalAccessException e) {
                        // TODO Auto-generated catch block
                        e.printStackTrace();
                    } catch (InvocationTargetException e) {
    
                    }
                } else if (transcriptionEvent2 != null) {
                    dispatchTranscriptionEvent(transcriptionThread.getUtterance(), transcriptionThread.getConfidence(), transcriptionThread.getStatus());
                }
                threads.remove(i);
            }
    
            if (debug && !status.equals(lastStatus)) {
                System.out.println(getTime() + " " + status);
                lastStatus = status;
            }
        }
    }
    
  #+END_SRC

  Call-back for the reduction-event is: =(lambda (hypothesis
  confidence) ...)=; register a series of parsers which either bite or
  pass on. Initially, though, just a parser. Or: one parser; multiple
  dispatchers? Yes.

  =jflac= is out of the question, since the encoder apparently [[https://github.com/hoenigmann/sicp.git][hasn't
  been implemented]]; the =javaFlacEncoder= has [[https://github.com/hoenigmann/sicp.git][FLACEncoder]] and
  [[https://github.com/hoenigmann/sicp.git][FLAC_FileEncoder]] (which Schultz used). The latter requires you to
  serialize wav, convert to FLAC, and send; the former is more complex
  to use, but can encode without serialization.

  We'll serialize to wav first; optimize later?

  HTTP-clients: [[http://hc.apache.org/][Apache commons]]; [[https://github.com/dakrone/clj-http][Clojure wrapper]]. [[http://hc.apache.org/httpcomponents-client-ga/tutorial/html/fundamentals.html#d4e199][Chunked encoding]] with
  name; [[http://hc.apache.org/httpcomponents-client-ga/httpclient/examples/org/apache/http/examples/client/ClientChunkEncodedPost.java][chunked encoding]] with POST. [[http://www.java-tips.org/other-api-tips/httpclient/how-to-use-multipart-post-method-for-uploading.html][Multi-part POST]]; where
  [[http://stackoverflow.com/questions/1067655/how-to-upload-a-file-using-java-httpclient-library-working-with-php-strange-pr][rebuketh]]. [[http://evgeny-goldin.com/blog/uploading-files-multipart-post-apache/][Writeup]] from Evgeny Goldin; referencing [[http://radomirml.com/2009/02/13/file-upload-with-httpcomponents-successor-of-commons-httpclient][this]] (which shows,
  by the way, how to upload from stream).

  ([[http://create.spinvox.com/][SpinVox]] as an alternative to Google, by the way.)

  Florian uses =file= as the parameter; the curl example uses
  =Content=: they both work.

  #+BEGIN_SRC java
    HttpClient client = new DefaultHttpClient();
    client.getParams().setParameter(CoreProtocolPNames.PROTOCOL_VERSION, HttpVersion.HTTP_1_1);
     
    HttpPost        post   = new HttpPost( url );
    MultipartEntity entity = new MultipartEntity( HttpMultipartMode.BROWSER_COMPATIBLE );
     
    // For File parameters
    entity.addPart( paramName, new FileBody((( File ) paramValue ), "application/zip" ));
     
    // For usual String parameters
    entity.addPart( paramName, new StringBody( paramValue.toString(), "text/plain",
                                               Charset.forName( "UTF-8" )));
     
    post.setEntity( entity );
     
    // Here we go!
    String response = EntityUtils.toString( client.execute( post ).getEntity(), "UTF-8" );
     
    client.getConnectionManager().shutdown();
  #+END_SRC

  #+BEGIN_SRC java :tangle post-to-google.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/httpcore-4.2-alpha2.jar");
    addClassPath("lib/httpclient-4.2-alpha1.jar");
    addClassPath("lib/httpmime-4.2-alpha1.jar");
    addClassPath("lib/commons-logging-1.1.1.jar");
    addClassPath("lib/gson-2.0.jar");
    
    import java.io.File;
    
    import org.apache.http.HttpVersion;
    import org.apache.http.client.methods.HttpPost;
    import org.apache.http.entity.mime.HttpMultipartMode;
    import org.apache.http.entity.mime.MultipartEntity;
    import org.apache.http.entity.mime.content.FileBody;
    import org.apache.http.entity.mime.content.StringBody;
    import org.apache.http.impl.client.DefaultHttpClient;
    import org.apache.http.params.CoreProtocolPNames;
    import org.apache.http.util.EntityUtils;
    
    client = new DefaultHttpClient();
    client.getParams().setParameter(CoreProtocolPNames.PROTOCOL_VERSION,
                                    HttpVersion.HTTP_1_1);
    post = new HttpPost("https://www.google.com/speech-api/v1/recognize?xjerr=1&client=chromium&lang=en-US");
    post.addHeader("Content-type", "audio/x-flac; rate=8000");
    entity = new MultipartEntity(HttpMultipartMode.BROWSER_COMPATIBLE);
    entity.addPart("Content", new FileBody(new File("harro.flac"), "audio/x-flac"));
    post.setEntity(entity);
    response = EntityUtils.toString(client.execute(post).getEntity(), "UTF-8");
    print(response);
    client.getConnectionManager().shutdown();
  #+END_SRC

  With Gson, I think we've reached the limit of beanshell; can't
  seem to define adequate classes.

  #+BEGIN_SRC java :tangle parse-json.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/gson-2.0.jar");
    
    import com.google.gson.Gson;
    import com.google.gson.reflect.TypeToken;
    
    response = "{\"status\":0,\"id\":\"85afc1835bc8583519599abebfd99d81-1\",\"hypotheses\":[{\"utterance\":\"toyota\",\"confidence\":0.95395637}]}";
    
    public class Response {
        int status;
        String id;
        Hypothesis[] hypotheses;
    
        public class Hypothesis {
            String utterance;
            float confidence;
        }
    }
    
    new Gson().fromJson(response, Response.class);
    
  #+END_SRC

  Rudy mentioned some stuff over farmer's that I didn't capture;
  something about [[http://en.wikipedia.org/wiki/Root_mean_square][mean square]] (as opposed to root mean square) for
  establishing a threshold. More sophisticated models do a band-pass
  filter for (possibly gender-specific) frequencies. Have to ask him
  for clarity. The model of take-the-max over $n$ milliseconds (Rudy
  mentioned that 10-20 is legit, btw) is terrible when dealing with
  e.g. spikes.

  #+BEGIN_SRC clojure :tangle record.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    
    (add-classpath "lib/javaFlacEncoder-0.2.3.jar")
    (add-classpath "lib/debug-1.0.0-SNAPSHOT.jar")
    
    (use 'debug.core)
    
    (import '(javax.sound.sampled
              AudioFormat
              AudioSystem
              AudioInputStream
              AudioFileFormat
              AudioFileFormat$Type))
    (import '(java.util
              Timer
              TimerTask))
    (import '(java.io
              File))
    (import '(javaFlacEncoder
              FLAC_FileEncoder
              StreamConfiguration))
    
    (def ^:dynamic *input-index* 
      "Default index of the recording device; NB: this is a hack."
      1)
    
    (def ^:dynamic *format*
      (new AudioFormat 8000 16 1 true false))
    
    (let [mixer-info (get (AudioSystem/getMixerInfo) *input-index*)
          target (AudioSystem/getTargetDataLine *format* mixer-info)]
      ;; `with-open'?
      (.open target *format*)
      (.start target)
      (let [timer (new Timer)
            task (proxy [TimerTask] []
                   (run []
                     (.flush target)
                     (.stop target)
                     (.close target)
                     (.cancel timer)))]
        (.schedule timer task 3000))
      (let [input-stream (new AudioInputStream target)]
        (let [wave (new File "harro.wav")
              flac (new File "harro.flac")]
          (AudioSystem/write input-stream
                             AudioFileFormat$Type/WAVE
                             wave)
          (let [encoder (new FLAC_FileEncoder)]
            (.setStreamConfig encoder
                              (new StreamConfiguration
                                   1
                                   StreamConfiguration/DEFAULT_MIN_BLOCK_SIZE
                                   StreamConfiguration/DEFAULT_MAX_BLOCK_SIZE
                                   8000
                                   16))
            (.encode encoder wave flac)))))
    
  #+END_SRC

  #+BEGIN_SRC clojure :tangle post.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    
    (add-classpath "lib/debug-1.0.0-SNAPSHOT.jar")
    (add-classpath "lib/clj-http-0.2.6-SNAPSHOT-standalone.jar")
    (add-classpath "lib/data.json-0.1.3-SNAPSHOT.jar")
    (add-classpath "lib/lambda-1.0.1-SNAPSHOT.jar")
    (add-classpath "lib/cadr-1.0.0-SNAPSHOT-standalone.jar")
    
    (use 'clojure.java.io)
    (use 'debug.core)
    (use 'clj-http.client)
    (use 'slingshot.slingshot)
    (use 'clojure.data.json)
    (use 'lambda.core)
    (use 'cadr.core)
    
    (import 'java.util.Random)
    
    (let [random (new Random)]
      (def random-element
        (λ [list]
           (nth list (.nextInt random (count list))))))
    
    (def sort-hypotheses
      (λ [hypotheses]
         (sort-by (λ [hypothesis]
                     (let [{utterance :utterance confidence :confidence}
                           hypothesis]
                       confidence))
                  >
                  hypotheses)))
    
    (let [{status :status
           id :id
           hypotheses :hypotheses}
          (read-json "{\"status\":0,\"id\":\"23a535f8f07e4c36456156a6fdbfe260-1\",\"hypotheses\":[{\"utterance\":\"test 123\",\"confidence\":0.96106875}]}\n")
          {utterance :utterance
           confidence :confidence}
          ;; Should we sort by confidence and take the first?
          (random-element hypotheses)]
      (debug status id hypotheses utterance confidence))
    
    ;; (debug (:body
    ;;         (post "https://www.google.com/speech-api/v1/recognize?xjerr=1&client=chromium&lang=en-US"
    ;;               {:multipart [["Content" (file "harro.flac")]]
    ;;                :headers {"Content-type" "audio/x-flac; rate=8000"}})))
    
  #+END_SRC
